{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646811c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEEK-8 (MDP AND Dynamic prog)(Policy and value iteration)\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1',is_slippery=True)\n",
    "\n",
    "def calculate_state_value(env, current_state, value_matrix, discount_factor):\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "    action_values = np.zeros(shape=num_actions)\n",
    "    for action in range(num_actions):\n",
    "        for transition_prob, next_state, reward, done in env.env.P[current_state][action]:\n",
    "            action_values[action] += transition_prob * (reward + discount_factor * value_matrix[next_state])\n",
    "    return action_values\n",
    "\n",
    "def evaluate_policy(policy_matrix, environment, discount_factor=1.0, convergence_threshold=1e-9, max_iterations=1000):\n",
    "    num_states = environment.observation_space.n\n",
    "    evaluation_iterations = 1\n",
    "    state_values = np.zeros(shape=num_states)\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0\n",
    "        for current_state in range(num_states):\n",
    "            new_state_value = 0\n",
    "            for action, action_probability in enumerate(policy_matrix[current_state]):\n",
    "                for state_probability, next_state, reward, done in environment.P[current_state][action]:\n",
    "                    new_state_value += action_probability * state_probability * (reward + discount_factor * state_values[next_state])\n",
    "            delta = max(delta, np.abs(state_values[current_state] - new_state_value))\n",
    "            state_values[current_state] = new_state_value\n",
    "\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'Policy evaluation terminated after {evaluation_iterations} iterations.\\n')\n",
    "            return state_values\n",
    "\n",
    "        \n",
    "def policy_iteration_algorithm(environment, discount_factor=1.0, max_iterations=1000):\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    policy_matrix = np.ones(shape=[num_states, num_actions]) / num_actions\n",
    "    evaluated_policies_count = 1\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        stable_policy = False\n",
    "        value_function = evaluate_policy(policy_matrix, environment, discount_factor)\n",
    "\n",
    "        for current_state in range(num_states):\n",
    "            current_action = np.argmax(policy_matrix[current_state])\n",
    "            action_values = calculate_state_value(environment, current_state, value_function, discount_factor)\n",
    "            best_action = np.argmax(action_values)\n",
    "\n",
    "            if current_action != best_action:\n",
    "                stable_policy = True\n",
    "\n",
    "            policy_matrix[current_state] = np.eye(num_actions)[best_action]\n",
    "\n",
    "        evaluated_policies_count += 1\n",
    "\n",
    "        if stable_policy:\n",
    "            print(f'Found a stable policy after {evaluated_policies_count:,} evaluations.\\n')\n",
    "            return policy_matrix, value_function\n",
    "\n",
    "        \n",
    "def value_iteration_algorithm(environment, discount_factor=1e-1, convergence_threshold=1e-9, max_iterations=1e4):\n",
    "    state_values = np.zeros(environment.observation_space.n)\n",
    "\n",
    "    for iteration in range(int(max_iterations)):\n",
    "        delta = 0\n",
    "\n",
    "        for current_state in range(environment.observation_space.n):\n",
    "            action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "            best_action_value = np.max(action_values)\n",
    "            delta = max(delta, np.abs(state_values[current_state] - best_action_value))\n",
    "            state_values[current_state] = best_action_value\n",
    "\n",
    "        if delta < convergence_threshold:\n",
    "            print(f'\\nValue iteration converged at iteration #{iteration+1:,}')\n",
    "            break\n",
    "\n",
    "    policy_matrix = np.zeros(shape=[environment.observation_space.n, environment.action_space.n])\n",
    "\n",
    "    for current_state in range(environment.observation_space.n):\n",
    "        action_values = calculate_state_value(environment, current_state, state_values, discount_factor)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy_matrix[current_state, best_action] = 1.0\n",
    "\n",
    "    return policy_matrix, state_values\n",
    "\n",
    "\n",
    "def play_episodes_and_evaluate(env, num_episodes, policy_matrix, max_actions=100, render=False):\n",
    "\n",
    "    total_wins = 0\n",
    "    total_rewards, total_actions = 0, 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        current_state = env.reset()\n",
    "        episode_done, actions_taken = False, 0\n",
    "\n",
    "        while actions_taken < max_actions:\n",
    "            selected_action = np.argmax(policy_matrix[current_state])\n",
    "            next_state, reward, episode_done, _ = env.step(selected_action)\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            actions_taken += 1\n",
    "            total_rewards += reward\n",
    "            current_state = next_state\n",
    "\n",
    "            if episode_done:\n",
    "                total_wins += 1\n",
    "                break\n",
    "\n",
    "        total_actions += actions_taken\n",
    "\n",
    "    print(f'Total rewards: {total_rewards:,}\\tMax actions: {actions_taken:,}')\n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    average_actions = total_actions / num_episodes\n",
    "\n",
    "    print('')\n",
    "    return total_wins, total_rewards, average_reward, average_actions\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "def agent_and_evaluate(env):\n",
    "\n",
    "    total_rewards_list = []\n",
    "\n",
    "    action_mapping = {\n",
    "        0: '\\u2191',  # up\n",
    "        1: '\\u2192',  # right\n",
    "        2: '\\u2193',  # down\n",
    "        3: '\\u2190'   # left\n",
    "    }\n",
    "\n",
    "    iteration_methods = [\n",
    "        ('Policy Iteration', policy_iteration_algorithm),\n",
    "        ('Value Iteration', value_iteration_algorithm)\n",
    "    ]\n",
    "\n",
    "    for method_name, method_func in iteration_methods:\n",
    "        policy_matrix, value_function = method_func(env)\n",
    "\n",
    "        print(f'Final policy using {method_name}:')\n",
    "        print(' '.join([action_mapping[action] for action in np.argmax(policy_matrix, axis=1)]))\n",
    "\n",
    "        total_wins, total_rewards, avg_reward, avg_actions = play_episodes_and_evaluate(env, num_episodes, policy_matrix)\n",
    "        total_rewards_list.append(total_rewards)\n",
    "\n",
    "        print(f'Number of wins = {total_wins:,}')\n",
    "        print(f'Average reward = {avg_reward:.2f}')\n",
    "        print(f'Average actions = {avg_actions:.2f}')\n",
    "\n",
    "    return total_rewards_list\n",
    "\n",
    "\n",
    "rewards = agent_and_evaluate(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEEK-9 (MDP and Monte carlo methods)\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "def monte_carlo_es(env, n_episodes=500):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    gamma = 1.0\n",
    "    total_steps = []\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        # generate an episode using exploring starts\n",
    "        while not done:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        total_steps.append(steps)\n",
    "\n",
    "        # update Q values using the episode\n",
    "        returns = 0\n",
    "        for j in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[j]\n",
    "            returns = gamma*returns + reward\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (returns - Q[state][action])/N[state][action]\n",
    "\n",
    "    # derive optimal policy from Q values\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "\n",
    "    return policy, Q, total_steps\n",
    "\n",
    "def on_policy_mc_control(env, n_episodes=500, epsilon=0.1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    gamma = 1.0\n",
    "    total_steps = []\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        # generate an episode using Ɛ-soft policy\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (reward + gamma*np.max(Q[next_state]) - Q[state][action])/N[state][action]\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        total_steps.append(steps)\n",
    "\n",
    "    # derive optimal policy from Q values\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "\n",
    "    return policy, Q, total_steps\n",
    "\n",
    "monte_carlo_es_policy, monte_carlo_es_q, total_steps_es = monte_carlo_es(env)\n",
    "on_policy_mc_control_policy, on_policy_mc_control_q, total_steps_control = on_policy_mc_control(env)\n",
    "\n",
    "print(str.format('Total Number of Steps taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)))\n",
    "print(str.format('Total Number of Steps taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)))\n",
    "\n",
    "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)/len(total_steps_es)))\n",
    "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)/len(total_steps_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEEK-10 (Temporal difference SARSA learning And Q-learning)\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "alpha = 0.4\n",
    "gamma = 0.9\n",
    "epsilon = 0.9\n",
    "num_episodes = 2000\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def sarsa():\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = epsilon_greedy_policy(next_state, epsilon)\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "    return rewards\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "def q_learning():\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = epsilon_greedy_policy(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "    return rewards\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "def expected_sarsa():\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = epsilon_greedy_policy(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            expected_value = np.dot(Q[next_state, :], np.ones(env.action_space.n) * epsilon / env.action_space.n) + \\\n",
    "                             np.max(Q[next_state, :]) * (1 - epsilon)\n",
    "            Q[state, action] += alpha * (reward + gamma * expected_value - Q[state, action])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                break\n",
    "    return rewards\n",
    "\n",
    "r_sarsa = sarsa()\n",
    "r_qlearn = q_learning()\n",
    "r_esarsa = expected_sarsa()\n",
    "\n",
    "plt.plot(r_sarsa)\n",
    "plt.title('Rewards generated by SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "np.mean(r_sarsa)\n",
    "\n",
    "plt.plot(r_qlearn)\n",
    "plt.title('Rewards generated by Q-Learning')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "np.mean(r_qlearn)\n",
    "\n",
    "plt.plot(r_esarsa)\n",
    "plt.title('Rewards generated by E-SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "np.mean(r_esarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
